{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otC0i9nsOwL2",
        "outputId": "d712558e-08c6-4272-92b9-3effb63b6040"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
        "import random\n",
        "import re\n",
        "\n",
        "seed = 10\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "AVHrOlrVeb0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6a9c1a2-267e-4018-9392-36992ca09bcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to find P(Y=k), just do test_labels.where(k)/len(test_labels)\n",
        "# vectos_train will contain all the information I need to create the counting for each word\n",
        "# use the get_feature_names to create a dictionary to map to all counts\n",
        "# this dictionary would be contained in another dict that has the labels as keys\n",
        "# to find P(xj,k), find the indices where test_labels = (k), then find"
      ],
      "metadata": {
        "id": "ILJgBzQUkOfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Handle cases where two examples are permutations of another\n",
        "# TODO: Handle laplace smoothing, i.e. word from corpus not in word list"
      ],
      "metadata": {
        "id": "tyztECAW9XOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worth looking into stripping accents, and sklearn feature extraction libarary including idf extractor"
      ],
      "metadata": {
        "id": "YeTjF93jXFiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Olympus in the Sky/McGill/2024 - Winter/ECSE 551/Data/'\n",
        "df = pd.read_csv(path + 'train.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "training_data = df['body'].apply(lambda x: x.replace('_', ' ')).to_numpy()\n",
        "training_labels = df['subreddit'].to_numpy()\n",
        "\n",
        "indices = np.random.permutation(len(training_data))\n",
        "training_data = training_data[indices]\n",
        "training_labels = training_labels[indices]\n",
        "\n",
        "test_split = 0.2\n",
        "\n",
        "(training_data, testing_data, training_labels, testing_labels) = train_test_split(training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "\n",
        "spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')"
      ],
      "metadata": {
        "id": "FKdP9D1nO2SM"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LemmaTokenizer:\n",
        "     def __init__(self):\n",
        "       self.wnl = WordNetLemmatizer()\n",
        "     def __call__(self, doc):\n",
        "       return [self.wnl.lemmatize(t,pos =\"v\") for t in word_tokenize(doc) if t.isalpha()]"
      ],
      "metadata": {
        "id": "VQCDOAUP_cvk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NB():\n",
        "  def __init__(self):\n",
        "    spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "    nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "    # self.vectorizer = CountVectorizer(binary = True, max_features = 3000, stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list)), tokenizer=LemmaTokenizer())\n",
        "    self.vectorizer = CountVectorizer(binary = True, max_features = 3000, stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list)))\n",
        "    self.no_pred = []\n",
        "\n",
        "  # currently train_corpus expects a np array containing a list of strings, not array of words, nxm\n",
        "  # test_label is an nx1 array\n",
        "  def train(self, train_corpus, test_labels):\n",
        "    assert len(train_corpus)  == len(test_labels)\n",
        "    assert type(train_corpus) == type(test_labels) == np.ndarray\n",
        "\n",
        "    vectors_train    = self.vectorizer.fit_transform(train_corpus).todense()\n",
        "    self.word_list   = self.vectorizer.get_feature_names_out()\n",
        "    self.num_samples = len(test_labels)\n",
        "    self.word_count  = dict(zip(self.word_list, np.array(vectors_train.sum(axis=0))[0]))\n",
        "\n",
        "    unique, counts   = np.unique(test_labels, return_counts=True)\n",
        "    self.label_count = dict(zip(unique, counts))\n",
        "    self.labels      = unique\n",
        "\n",
        "    self.word_count_given_label = {}\n",
        "    for label in self.labels:\n",
        "      indices = np.where(test_labels == label)[0]\n",
        "      tot_word_count = np.array(vectors_train[indices].sum(axis=0))[0]\n",
        "      self.word_count_given_label[label] = {self.word_list[i] : tot_word_count[i] for i in range(len(tot_word_count))}\n",
        "      # print(label, self.word_count_given_label[label])\n",
        "\n",
        "  # Assuming test_corpus is 2-d array where each test sample is a string\n",
        "  def predict(self, test_corpus):\n",
        "\n",
        "    predictions = []\n",
        "    pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
        "    for index, corpus in enumerate(test_corpus):\n",
        "\n",
        "      best_label = ''\n",
        "      best_prob  = -np.inf\n",
        "\n",
        "      # Text processing based on CountVectorizer's regex\n",
        "      # corpus = corpus.lower().replace('_', ' ')\n",
        "      # corpus_words = list(set(re.findall(pattern, corpus)))\n",
        "\n",
        "      corpus = corpus.replace('_', ' ')\n",
        "      try:\n",
        "        _ = self.vectorizer.fit_transform([corpus])\n",
        "      except:\n",
        "        self.no_pred.append(corpus)\n",
        "        best_label = random.choice(self.labels)\n",
        "        predictions.append(best_label)\n",
        "        continue\n",
        "      corpus_words  = self.vectorizer.get_feature_names_out()\n",
        "\n",
        "      for label in self.labels:\n",
        "        p_of_y = self.label_count[label]/self.num_samples\n",
        "\n",
        "        p_of_x_given_y = 1\n",
        "\n",
        "        for word in self.word_list:\n",
        "          if word in corpus_words:\n",
        "            xj = 1\n",
        "          else:\n",
        "            xj = 0\n",
        "\n",
        "          theta_xj_k      = (self.word_count_given_label[label][word] + 1) / (self.label_count[label] + len(self.labels))\n",
        "          p_of_x_given_y *= (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj))\n",
        "          # print(label, word, p_of_x_given_y, theta_xj_k, (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj)))\n",
        "\n",
        "        unseen_words = [new_word for new_word in corpus_words if new_word not in self.word_list]\n",
        "        for word in unseen_words:\n",
        "          # p_of_x_given_y *= 1/(self.label_count[label] + len(self.labels))\n",
        "          p_of_x_given_y *= 1/(len(self.labels)) # When there are a large number of unseen words, the p_of_x_given_y basically becomes 0\n",
        "        # print(len(unseen_words), len(corpus_words))\n",
        "\n",
        "        p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
        "        # print(p_of_y, p_of_x_given_y)\n",
        "        # print(label, p_of_y_given_x)\n",
        "\n",
        "        # print(p_of_y_given_x, best_prob)\n",
        "        if p_of_y_given_x > best_prob:\n",
        "          best_prob = p_of_y_given_x\n",
        "          best_label = label\n",
        "\n",
        "      if best_label == '':\n",
        "        self.no_pred.append(corpus)\n",
        "      #   best_label = random.choice(self.labels)\n",
        "\n",
        "      predictions.append(best_label)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "tJypvj7ZnfWY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_validation(x_train_data, y_train_data, K = 10):\n",
        "  assert type(x_train_data) == np.ndarray and type(y_train_data) == np.ndarray\n",
        "\n",
        "  len_of_data  = len(x_train_data) - len(x_train_data) % K\n",
        "  data_x       = x_train_data[:len_of_data]\n",
        "  data_y       = y_train_data[:len_of_data]\n",
        "  size_of_fold = len(x_train_data[:len_of_data]) // K\n",
        "  validation_error = 0\n",
        "\n",
        "  error    = []\n",
        "  pred     = []\n",
        "  timings  = []\n",
        "  no_preds = []\n",
        "\n",
        "  naive_bayes = NB()\n",
        "\n",
        "  for i in range(K):\n",
        "\n",
        "    naive_bayes = NB()\n",
        "\n",
        "    if i != K-1:\n",
        "      validation_fold_x = data_x[i*size_of_fold:(i+1)*size_of_fold]\n",
        "      validation_fold_y = data_y[i*size_of_fold:(i+1)*size_of_fold]\n",
        "\n",
        "      training_folds_x  = np.concatenate((data_x[:i*size_of_fold], data_x[(i+1)*size_of_fold:]))\n",
        "      training_folds_y  = np.concatenate((data_y[:i*size_of_fold], data_y[(i+1)*size_of_fold:]))\n",
        "\n",
        "    else:\n",
        "      validation_fold_x = data_x[i*size_of_fold:]\n",
        "      validation_fold_y = data_y[i*size_of_fold:]\n",
        "\n",
        "      training_folds_x  = data_x[:i*size_of_fold]\n",
        "      training_folds_y  = data_y[:i*size_of_fold]\n",
        "\n",
        "    start_time = time.time()\n",
        "    naive_bayes.train(training_folds_x, training_folds_y)\n",
        "    end_time = time.time()\n",
        "\n",
        "    timings.append(end_time - start_time)\n",
        "\n",
        "    pred_valid = naive_bayes.predict(validation_fold_x)\n",
        "    no_pred = naive_bayes.no_pred\n",
        "    pred_train = naive_bayes.predict(training_folds_x)\n",
        "\n",
        "    fold_error = {}\n",
        "    fold_error['validation'] = 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    validation_error        += 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    fold_error['train']      = 1 - accuracy_score(pred_train, training_folds_y)\n",
        "\n",
        "    model_pred = {}\n",
        "    model_pred['validation'] = (pred_valid, validation_fold_y)\n",
        "    model_pred['train']      = (pred_train, training_folds_y )\n",
        "\n",
        "    error.append(fold_error)\n",
        "    pred.append(model_pred)\n",
        "    no_preds.append(no_pred)\n",
        "\n",
        "  info = {'error': error, 'pred': pred, 'time': timings, 'no_preds' : no_preds}\n",
        "\n",
        "  return validation_error/K, info"
      ],
      "metadata": {
        "id": "tb85vAgDPPdC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple test cases for NB"
      ],
      "metadata": {
        "id": "7xBVjFXWpw7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "     'Is this the first document?',]\n",
        "test_labels = np.array(['1', '2', '3', '4'])\n",
        "train_corpus = np.array(train_corpus)\n",
        "test_corpus = ['harro first pink haha']\n",
        "model = NB()\n",
        "model.train(train_corpus, test_labels)\n",
        "model.predict(test_corpus)"
      ],
      "metadata": {
        "id": "qo7BKNgek7NV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae6c5794-69ec-4377-be3a-f8bb99ff2d6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "X_train = np.array([\n",
        "    \"I love this movie\",\n",
        "    \"This movie is great\",\n",
        "    \"A movie like this is great\",\n",
        "    \"I hate this movie\",\n",
        "    \"This movie is terrible\"\n",
        "])\n",
        "\n",
        "# Corresponding labels\n",
        "y_train = np.array([1, 1, 1, 0, 0])  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "\n",
        "model = NB()\n",
        "model.train(X_train, y_train)\n",
        "X_test = [\n",
        "    \"I love this movie terrible\", # Example of both class being viable but the second being chosen due to label eval order in NB\n",
        "    \"I hate this great movie\",\n",
        "    \"This movie is terrible\"\n",
        "]\n",
        "\n",
        "\n",
        "print(model.predict(X_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jmTZkw2lgUe",
        "outputId": "ba028b4c-a3de-4c9c-947f-508ff83bf84e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-fold for NB"
      ],
      "metadata": {
        "id": "PlHr32RYp0Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: How to select best performing split\n",
        "# TODO: No predictions given, divide by zero warnings might be the cause (Solved?)\n",
        "# TODO: Prune weird words from word lsit like numebrs and etc\n",
        "# TODO: Add lemitization\n",
        "# TODO: Test changing the prob caluclation to be a sum of logs\n",
        "# TODO: Lemmitization for french and english"
      ],
      "metadata": {
        "id": "eTGtkcsZM7FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NB()\n",
        "model.train(training_data, training_labels)\n",
        "pred = model.predict(testing_data)\n",
        "(pred == testing_labels).sum()/len(testing_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVQVLywgFjE1",
        "outputId": "1ccd7386-aa03-43b3-ef76-6f35427cbb6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6357142857142857"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = model.no_pred\n",
        "\n",
        "debug = NB()\n",
        "debug.train(training_data, training_labels)\n",
        "pred = debug.predict(data)\n",
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pHQuYFTA_N0",
        "outputId": "aeabd792-ead0-47c0-8d6d-b16b402e23d9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error, info = k_fold_validation(training_data, training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbhJpJG3p1rR",
        "outputId": "d6e3bd69-fc26-4d2d-99b0-cdd51fa4bb62"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiSEAKEGKUxu",
        "outputId": "61787131-8a53-40af-adcf-f16c8b206e78"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3589285714285714"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info['error']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdAVZHF1H25p",
        "outputId": "20ca09d8-ce5a-4d67-c375-61035e2f23db"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'validation': 0.3392857142857143, 'train': 0.08730158730158732},\n",
              " {'validation': 0.3035714285714286, 'train': 0.08730158730158732},\n",
              " {'validation': 0.4196428571428571, 'train': 0.10317460317460314},\n",
              " {'validation': 0.4017857142857143, 'train': 0.0982142857142857},\n",
              " {'validation': 0.3303571428571429, 'train': 0.08333333333333337},\n",
              " {'validation': 0.3660714285714286, 'train': 0.09226190476190477},\n",
              " {'validation': 0.4464285714285714, 'train': 0.09523809523809523},\n",
              " {'validation': 0.3482142857142857, 'train': 0.10515873015873012},\n",
              " {'validation': 0.3214285714285714, 'train': 0.09126984126984128},\n",
              " {'validation': 0.3125, 'train': 0.08234126984126988}]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info['no_preds']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85YLhTARLmvg",
        "outputId": "ef449ccf-e198-4151-aa1b-c32da9eeab9f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????'],\n",
              " ['?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????? ?????????????????????']]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.word_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mopWnJFtmoyk",
        "outputId": "e2e53302-0b52-480d-c75f-d9df63d850f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '02', ..., 'zero', 'zone', '每每'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(model.word_count.items(), key=lambda item: item[1], reverse = True)[:-1]"
      ],
      "metadata": {
        "id": "Z66nKW_EmqiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual Test Case"
      ],
      "metadata": {
        "id": "Giz98mOOps5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NB()\n",
        "model.train(training_data, training_labels)\n",
        "\n",
        "test_df = pd.read_csv(path + 'test.csv', encoding = \"ISO-8859-1\")\n",
        "test_data = test_df['body'].to_numpy()\n",
        "\n",
        "predictions = model.predict(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W85aadzmO69b",
        "outputId": "5c5f82ad-ef5e-4170-8f8d-941977205f0c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'Subreddit':predictions}).reset_index().rename(columns={\"index\": \"Id\"}).to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "YHux80f5QvTq"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "2gOAtTukpq3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja\n",
        "import wordninja"
      ],
      "metadata": {
        "id": "Gj5I-SXfTlDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(wordninja.split('This document is the spider-man document,egxnd3mtd2l6lxnlcnaih0j1cybhbmqgq29hy2ggq2hhcnrlcibtzwxib3vybmuybhaagbyyhjileaaygaqyiguyhgmycxaagiaegiofgiydmgsqabiabbikbrigazileaaygaqyiguyhgmycxaagiaegiofgiydsoobuksdwkkbcaf4azabajgbnwkgaewnqgedmi04uaedyaea.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ci1o7tAvUHpO",
        "outputId": "145f66a3-cd8f-4d97-bde9-8b4a1ca01d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This document is the spider man document eg x nd 3 m td 2 l 6 l xn lc nai h 0 j 1 cy bh bm qg q 29 hy 2 g gq 2 h hcn rlc ibt zw xi b 3 vy b muy b haag by y hj ilea a yg aq yi guy hg my cx a agia eg i of gi yd mg sq abi abb ik brig a zile a a yg aq yi guy hg my cx a agia eg i of gi yd soo buk s dw k kb caf 4 az abaj gb nw kg a ew n qg edm i 04 uae dy a ea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(model.word_count.items(), key=lambda item: item[1], reverse = True)[-100:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyKywPkeZVxf",
        "outputId": "39e16395-e602-4c13-acf5-2438bcf8f3fd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('target', 3),\n",
              " ('tarif', 3),\n",
              " ('tarifs', 3),\n",
              " ('tea', 3),\n",
              " ('tech', 3),\n",
              " ('technically', 3),\n",
              " ('tends', 3),\n",
              " ('terminal', 3),\n",
              " ('thermique', 3),\n",
              " ('thoughts', 3),\n",
              " ('thousand', 3),\n",
              " ('thrown', 3),\n",
              " ('til', 3),\n",
              " ('toast', 3),\n",
              " ('toilets', 3),\n",
              " ('tol', 3),\n",
              " ('tool', 3),\n",
              " ('tops', 3),\n",
              " ('totally', 3),\n",
              " ('tottenham', 3),\n",
              " ('tough', 3),\n",
              " ('towns', 3),\n",
              " ('trajet', 3),\n",
              " ('trans', 3),\n",
              " ('trash', 3),\n",
              " ('travailleurs', 3),\n",
              " ('travelled', 3),\n",
              " ('trees', 3),\n",
              " ('triomphe', 3),\n",
              " ('trottinette', 3),\n",
              " ('trouvent', 3),\n",
              " ('trucks', 3),\n",
              " ('trump', 3),\n",
              " ('tudiants', 3),\n",
              " ('tunnel', 3),\n",
              " ('turning', 3),\n",
              " ('typical', 3),\n",
              " ('ues', 3),\n",
              " ('ukraine', 3),\n",
              " ('understood', 3),\n",
              " ('unemployed', 3),\n",
              " ('units', 3),\n",
              " ('universit', 3),\n",
              " ('unlikely', 3),\n",
              " ('unpopular', 3),\n",
              " ('unsure', 3),\n",
              " ('update', 3),\n",
              " ('ups', 3),\n",
              " ('urbain', 3),\n",
              " ('urban', 3),\n",
              " ('urgence', 3),\n",
              " ('urgent', 3),\n",
              " ('users', 3),\n",
              " ('ussi', 3),\n",
              " ('valeur', 3),\n",
              " ('valuable', 3),\n",
              " ('vast', 3),\n",
              " ('venue', 3),\n",
              " ('verbal', 3),\n",
              " ('viable', 3),\n",
              " ('victimes', 3),\n",
              " ('vid', 3),\n",
              " ('vidanges', 3),\n",
              " ('vides', 3),\n",
              " ('vieux', 3),\n",
              " ('views', 3),\n",
              " ('virgin', 3),\n",
              " ('visa', 3),\n",
              " ('visible', 3),\n",
              " ('visibly', 3),\n",
              " ('vivent', 3),\n",
              " ('vmc', 3),\n",
              " ('voies', 3),\n",
              " ('voit', 3),\n",
              " ('volume', 3),\n",
              " ('volunteer', 3),\n",
              " ('voted', 3),\n",
              " ('voulez', 3),\n",
              " ('wanna', 3),\n",
              " ('watched', 3),\n",
              " ('weaver', 3),\n",
              " ('wednesday', 3),\n",
              " ('weights', 3),\n",
              " ('western', 3),\n",
              " ('wiki', 3),\n",
              " ('wikipedia', 3),\n",
              " ('win', 3),\n",
              " ('windy', 3),\n",
              " ('wise', 3),\n",
              " ('wonderful', 3),\n",
              " ('worker', 3),\n",
              " ('worried', 3),\n",
              " ('worthwhile', 3),\n",
              " ('write', 3),\n",
              " ('yard', 3),\n",
              " ('yarra', 3),\n",
              " ('yelling', 3),\n",
              " ('yellow', 3),\n",
              " ('zero', 3),\n",
              " ('每每', 3)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awAAMg5wSCL8"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otC0i9nsOwL2",
        "outputId": "21d130d3-7d58-44eb-9de4-d6497539b395"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVHrOlrVeb0y",
        "outputId": "f7951e7d-bd74-4a09-a8bf-2b45bf893d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
            "  Cloning https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git to c:\\users\\admin\\appdata\\local\\temp\\pip-req-build-hxdzh6o8\n",
            "  Resolved https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git to commit bc0ebd0135a6cc78f48ddf184069b4c0b9c017d8\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Installing backend dependencies: started\n",
            "  Installing backend dependencies: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  Running command git clone --filter=blob:none --quiet https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git 'C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-req-build-hxdzh6o8'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.0.9)\n",
            "Requirement already satisfied: six in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langdetect) (1.16.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.66.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "# from nltk.stem import French\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
        "!pip install git+https://github.com/ClaudeCoulombe/FrenchLefffLemmatizer.git\n",
        "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer # package for french lemmatization\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "import random\n",
        "import re\n",
        "!pip install langdetect\n",
        "from nltk.corpus import wordnet\n",
        "from langdetect import detect\n",
        "!pip install tqdm\n",
        "from tqdm import tqdm\n",
        "# from english_dictionary.scripts.read_pickle import get_dict\n",
        "from english_words import get_english_words_set\n",
        "\n",
        "seed = 10\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LEKGk_qnSGsc"
      },
      "source": [
        "## Get Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZzV4brs6SAN3"
      },
      "outputs": [],
      "source": [
        "#path = '/content/drive/MyDrive/Olympus in the Sky/McGill/2024 - Winter/ECSE 551/Data/'\n",
        "path = 'Data/'\n",
        "df = pd.read_csv(path + 'train.csv', encoding = \"ISO-8859-1\")\n",
        "df = df.sample(frac=1, random_state=1).reset_index(drop = True)\n",
        "df = df[~df['body'].str.contains('\\?'*10)]\n",
        "\n",
        "training_data = df['body'].apply(lambda x: x.lower().replace('_', ' ')).to_numpy()\n",
        "training_labels = df['subreddit'].to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmqvpiUNIPqw"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "06PcDk3Dz-Iz"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "  \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "  tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "  tag_dict = {\"J\": wordnet.ADJ,\n",
        "              \"N\": wordnet.NOUN,\n",
        "              \"V\": wordnet.VERB,\n",
        "              \"R\": wordnet.ADV}\n",
        "  return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def remove_stop_words(arr_of_words, stop_words):\n",
        "  return [word for word in arr_of_words if word not in stop_words]\n",
        "\n",
        "\n",
        "def remove_invalid_words(text, valid_words):\n",
        "    return [word for word in text if word in valid_words]\n",
        "\n",
        "def lemmatize(arr_of_words):\n",
        "  lang = detect(' '.join(arr_of_words))\n",
        "  if lang == 'fr':\n",
        "      arr_of_words.append('french')\n",
        "      lemmatizer = FrenchLefffLemmatizer()\n",
        "  else:\n",
        "      arr_of_words.append('english')\n",
        "      lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  return [lemmatizer.lemmatize(word, pos = get_wordnet_pos(word)) for word in arr_of_words]\n",
        "\n",
        "def preprocess(sentences, regex_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", stop_words = None, is_lemmatize = False, validate_words = None):\n",
        "  new_sentence_list = []\n",
        "  pattern = regex_pattern\n",
        "\n",
        "  for iter, sentence in enumerate(sentences):\n",
        "\n",
        "      words = list(set(re.findall(pattern, sentence)))\n",
        "      if stop_words:\n",
        "        words = remove_stop_words(words, stop_words)\n",
        "\n",
        "      if is_lemmatize:\n",
        "        words = lemmatize(words)\n",
        "\n",
        "      # if validate_words: \n",
        "      #    words = remove_invalid_words(words, validate_words)\n",
        "\n",
        "      new_sentence_list.append(' '.join(words))\n",
        "\n",
        "  return np.array(new_sentence_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['London' 'London' 'London']\n",
            "Accuracy: 0.3333333333333333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import SGDClassifier\n",
        "# from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import svm \n",
        "\n",
        "class SVM(): \n",
        "    def __init__(self, max_features = 3000, token_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", min_df = 1, max_df = 1.0, ngram_range = (1,1)):\n",
        "        spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "        nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "        other_stop_words = ['http', 'https', 'www']\n",
        "        stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list, other_stop_words))\n",
        "        self.pattern = token_pattern\n",
        "        self.ngram_range = ngram_range\n",
        "        self.vectorizer = TfidfVectorizer(binary = True, max_features = max_features, token_pattern = self.pattern, min_df = min_df, max_df = max_df, ngram_range = ngram_range)\n",
        "        self.svm_class = svm.SVC(kernel='linear', random_state=seed, degree=3)\n",
        "\n",
        "    def train(self, x_data_train, x_data_labels):\n",
        "        assert len(x_data_train)  == len(x_data_labels)\n",
        "        # assert type(x_data_train) == type(x_data_labels) == np.ndarray\n",
        "        # fit the \n",
        "        vectors_train_idf = self.vectorizer.fit_transform(x_data_train)\n",
        "        self.svm_class.fit(vectors_train_idf, x_data_labels)\n",
        "    \n",
        "    def predict(self, x_data_test): \n",
        "        tfidf_test = self.vectorizer.transform(x_data_test)\n",
        "        x_data_pred = self.svm_class.predict(tfidf_test)\n",
        "        return x_data_pred\n",
        "\n",
        "\n",
        "X_train = [\n",
        "    \"On est bien d?accord que la question pos‚e ne contenait pas le mot SUV, mais bien les diff‚rents adjectifs, lourds,  encombrant, polluants. Sans ®ÿetÿ¯. Les voitures/camionettes (?), encombrantes ou  polluantes des pauvres (ou pas) seront bien tax‚es comme les 4x4?\",\n",
        "    \"You can thank the state governments soft approach on drugs and the injecting rooms for the proliferation of junkies all over the city. To save a few lives of junkies that add zero to society, they've destroyed the city for the rest of us. Let's not pretend that heroin users and ice users are mutually exclusive groups.\",\n",
        "    \"I?m not comparing them because I?m not clued up enough on either situation but I would love to understand how we are all so angry about what?s going on in Gaza but we all basically ignored the genocide in sudan? Is it because Sudan was a civil war and Gaza is attacked by a different state?\",\n",
        "    \"My group is mid to late 30s. We match our agendas and commit. We're pretty open about how we feel (tiredness, money, other commitments) but mainly we make an effort to show up because we enjoy hanging around each other. As an introvert i found friends who don't drain my energy so that also helps.\",\n",
        "    \"I was ready to be disappointed tbh (last years were misses for me) but I?m pleasantly surprised on this one! Arlo Parks, L‚onie Gray, Ren‚e Rapp (loved her on Broadway), Labrinth, T-Pain, Hozier, Jungle (great music fest vibes), Still Woozy, Raye?this is gonna be fun! Unsure if it?s 395$ fun though ??\"]\n",
        "\n",
        "y_train = [\"Paris\", \"Melbourne\", \"London\", \"London\", \"Montreal\"]  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "model = SVM()\n",
        "model.train(X_train, y_train)\n",
        "\n",
        "X_test = [\n",
        "    \"Dosa Pointe - If you can get it ! Often sells out and quickly Restaurant Canada Best - absolutely incredible, don't believe me look up the reviews Halal 786 - even just the plain Biryani rice is delicious beyond so many other things on the menu, have it weekly\",  \n",
        "    \"# Upvote/Downvote reminder Like this image or appreciate it being posted? Upvote it and show it some love! Don't like it? Just downvote and move on. *Upvoting or downvoting images it the best way to control what you see on your feed and what gets to the top of the subreddit* *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/london) if you have any questions or concerns.*\",\n",
        "    \"Les jardins du chateau de versailles. C'est pas une nature sauvage, mais vu la dimension des jardins, tu peux te retrouver dans des endroits o— aucun touriste n'osera s'aventurer. 15 minutes de train + 10 minutes de marche depuis la gare montparnasse. Id‚alement, avec un v‚lo...\"]\n",
        "\n",
        "Y_test = [\"Montreal\", \"London\", \"Paris\"]\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(y_pred)\n",
        "\n",
        "print(\"Accuracy:\",accuracy_score(Y_test, y_pred))\n",
        "\n",
        "\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Melbourne' 'Paris' 'Paris' 'Montreal' 'Melbourne' 'Melbourne' 'London'\n",
            " 'London' 'Melbourne' 'Melbourne' 'London' 'Melbourne' 'Montreal'\n",
            " 'Melbourne' 'Paris' 'Montreal' 'Melbourne' 'Melbourne' 'London'\n",
            " 'Montreal' 'Montreal' 'Montreal' 'Montreal' 'London' 'Melbourne'\n",
            " 'Montreal' 'London' 'Montreal' 'Melbourne' 'Montreal' 'Melbourne'\n",
            " 'Montreal' 'Melbourne' 'London' 'Melbourne' 'Paris' 'Melbourne' 'Paris'\n",
            " 'Paris' 'Melbourne' 'Paris' 'Paris' 'Melbourne' 'Melbourne' 'Melbourne'\n",
            " 'Montreal' 'Melbourne' 'London' 'Melbourne' 'Melbourne' 'London'\n",
            " 'Melbourne' 'Melbourne' 'Melbourne' 'Melbourne' 'Paris' 'London' 'London'\n",
            " 'Melbourne' 'Melbourne' 'Melbourne' 'London' 'Paris' 'Montreal'\n",
            " 'Melbourne' 'Montreal' 'Montreal' 'London' 'London' 'Melbourne'\n",
            " 'Melbourne' 'Montreal' 'Paris' 'Melbourne' 'Montreal' 'Melbourne'\n",
            " 'London' 'Melbourne' 'Melbourne' 'Paris' 'Melbourne' 'Paris' 'Melbourne'\n",
            " 'Montreal' 'Paris' 'Melbourne' 'Melbourne' 'Melbourne' 'Montreal' 'Paris'\n",
            " 'Melbourne' 'London' 'Montreal' 'Paris' 'Montreal' 'Paris' 'Montreal'\n",
            " 'Montreal' 'Melbourne' 'London' 'Montreal' 'Paris' 'Melbourne' 'London'\n",
            " 'Montreal' 'Melbourne' 'Melbourne' 'Melbourne' 'Montreal' 'Paris'\n",
            " 'Melbourne' 'Montreal' 'Paris' 'Melbourne' 'Montreal' 'Melbourne'\n",
            " 'Melbourne' 'London' 'Melbourne' 'London' 'Melbourne' 'Melbourne' 'Paris'\n",
            " 'London' 'Montreal' 'Montreal' 'Melbourne' 'Paris' 'Montreal' 'Melbourne'\n",
            " 'Paris' 'Melbourne' 'Paris' 'Melbourne' 'London' 'Montreal' 'Melbourne'\n",
            " 'Paris' 'London']\n",
            "Accuracy: 0.6834532374100719\n"
          ]
        }
      ],
      "source": [
        "# No preprocessing\n",
        "\n",
        "test_split = 0.1\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "\n",
        "model = SVM()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "print(pred)\n",
        "print(\"Accuracy:\",accuracy_score(testing_labels_split, pred))\n",
        "\n",
        "\n",
        "# mean_error, info = k_fold_validation(training_data_split, training_labels_split)\n",
        "# print('Mean Error: {}'.format(mean_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Melbourne' 'Paris' 'Paris' 'Montreal' 'Melbourne' 'London' 'London'\n",
            " 'London' 'Melbourne' 'London' 'London' 'Melbourne' 'Montreal' 'London'\n",
            " 'Montreal' 'Montreal' 'Melbourne' 'Melbourne' 'London' 'London'\n",
            " 'Montreal' 'Montreal' 'Montreal' 'London' 'Montreal' 'Montreal' 'London'\n",
            " 'Montreal' 'Melbourne' 'Montreal' 'Melbourne' 'Montreal' 'Melbourne'\n",
            " 'Melbourne' 'Melbourne' 'Paris' 'London' 'Paris' 'Paris' 'Melbourne'\n",
            " 'Montreal' 'Paris' 'Melbourne' 'Melbourne' 'Montreal' 'Montreal' 'London'\n",
            " 'London' 'Melbourne' 'Paris' 'London' 'Melbourne' 'Melbourne' 'Melbourne'\n",
            " 'Montreal' 'Paris' 'London' 'London' 'Montreal' 'Melbourne' 'Melbourne'\n",
            " 'London' 'Paris' 'Paris' 'Montreal' 'Montreal' 'Montreal' 'London'\n",
            " 'London' 'Melbourne' 'London' 'Montreal' 'Montreal' 'Melbourne'\n",
            " 'Montreal' 'Melbourne' 'Montreal' 'London' 'Melbourne' 'Paris'\n",
            " 'Melbourne' 'Paris' 'Melbourne' 'Montreal' 'Paris' 'Melbourne'\n",
            " 'Melbourne' 'Melbourne' 'Montreal' 'Montreal' 'Melbourne' 'London'\n",
            " 'Melbourne' 'Paris' 'Montreal' 'Paris' 'Montreal' 'Paris' 'Melbourne'\n",
            " 'London' 'Montreal' 'Montreal' 'Montreal' 'London' 'London' 'Montreal'\n",
            " 'Montreal' 'Montreal' 'Montreal' 'Paris' 'Melbourne' 'Montreal' 'Paris'\n",
            " 'Melbourne' 'Montreal' 'Melbourne' 'Melbourne' 'London' 'Melbourne'\n",
            " 'Paris' 'Melbourne' 'Melbourne' 'Paris' 'London' 'Montreal' 'Melbourne'\n",
            " 'Melbourne' 'Montreal' 'Montreal' 'Melbourne' 'Paris' 'London' 'Paris'\n",
            " 'Paris' 'London' 'Montreal' 'London' 'Paris' 'London']\n",
            "Accuracy: 0.60431654676259\n"
          ]
        }
      ],
      "source": [
        "# No preprocessing using countVectorizer \n",
        "test_split = 0.1\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "\n",
        "model = SVM()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "print(pred)\n",
        "print(\"Accuracy:\",accuracy_score(testing_labels_split, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.6402877697841727\n"
          ]
        }
      ],
      "source": [
        "# WithPreprocessing \n",
        "test_split = 0.1\n",
        "\n",
        "spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "other_stop_words = ['http', 'https', 'www']\n",
        "stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list, other_stop_words))\n",
        "\n",
        "# mots = set(line.strip() for line in open(path + 'dictionnaire.txt'))\n",
        "french_dict = open(path + 'dictionnaire.txt', \"r\", encoding = \"UTF-8\")\n",
        "\n",
        "mots = french_dict.read()\n",
        "\n",
        "french_mots = mots.replace('\\n', ' ').split(\".\")\n",
        "\n",
        "english_words = get_english_words_set(['web2'], lower=True)\n",
        "\n",
        "valid_words = list(set().union(english_words, french_mots))\n",
        "\n",
        "# df = pd.DataFrame(english_words)\n",
        "\n",
        "# df.to_csv('English dictionnary.csv')\n",
        "\n",
        "model = SVM()\n",
        "\n",
        "# X_train = [\n",
        "#     \"On est bien d?accord que la question pos‚e ne contenait pas le mot SUV, mais bien les diff‚rents adjectifs, lourds,  encombrant, polluants. Sans ®ÿetÿ¯. Les voitures/camionettes (?), encombrantes ou  polluantes des pauvres (ou pas) seront bien tax‚es comme les 4x4?\",\n",
        "#     \"You can thank the state governments soft approach on drugs and the injecting rooms for the proliferation of junkies all over the city. To save a few lives of junkies that add zero to society, they've destroyed the city for the rest of us. Let's not pretend that heroin users and ice users are mutually exclusive groups.\",\n",
        "#     \"I?m not comparing them because I?m not clued up enough on either situation but I would love to understand how we are all so angry about what?s going on in Gaza but we all basically ignored the genocide in sudan? Is it because Sudan was a civil war and Gaza is attacked by a different state?\",\n",
        "#     \"My group is mid to late 30s. We match our agendas and commit. We're pretty open about how we feel (tiredness, money, other commitments) but mainly we make an effort to show up because we enjoy hanging around each other. As an introvert i found friends who don't drain my energy so that also helps.\",\n",
        "#     \"I was ready to be disappointed tbh (last years were misses for me) but I?m pleasantly surprised on this one! Arlo Parks, L‚onie Gray, Ren‚e Rapp (loved her on Broadway), Labrinth, T-Pain, Hozier, Jungle (great music fest vibes), Still Woozy, Raye?this is gonna be fun! Unsure if it?s 395$ fun though ??\"]\n",
        "\n",
        "processed_training_data = preprocess(training_data, regex_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", stop_words = stop_words, is_lemmatize = True)\n",
        "\n",
        "# df = pd.DataFrame(processed_training_data)\n",
        "\n",
        "# df.to_csv(\"Processed Test.csv\")\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(processed_training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "# print(training_data_split)\n",
        "# model = SVM()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "# # print(pred)\n",
        "print(\"Accuracy:\",accuracy_score(testing_labels_split, pred))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_df = pd.read_csv(path + 'test.csv', encoding = \"ISO-8859-1\")\n",
        "test_data = test_df['body'].to_numpy()\n",
        "\n",
        "prediction = model.predict(test_data)\n",
        "\n",
        "# processed_test_data = lemmatize_and_remove_stop_words(test_data)\n",
        "\n",
        "# predictions = model.predict(processed_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "pd.DataFrame({'Subreddit':prediction}).reset_index().rename(columns={\"index\": \"Id\"}).to_csv('results_SVM.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE3SbVUlPho5"
      },
      "source": [
        "# NB and K-Fold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX4Psl8CSNg9"
      },
      "outputs": [],
      "source": [
        "# to find P(Y=k), just do test_labels.where(k)/len(test_labels)\n",
        "# vectos_train will contain all the information I need to create the counting for each word\n",
        "# use the get_feature_names to create a dictionary to map to all counts\n",
        "# this dictionary would be contained in another dict that has the labels as keys\n",
        "# to find P(xj,k), find the indices where test_labels = (k), then find"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "1SOK9cAT17fm"
      },
      "outputs": [],
      "source": [
        "def get_wordnet_pos(word):\n",
        "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "class LemmaTokenizer:\n",
        "     def __init__(self, pattern):\n",
        "       self.pattern = pattern\n",
        "     def __call__(self, doc):\n",
        "       doc = doc.lower().replace('_', ' ')\n",
        "       doc_words = list(set(re.findall(self.pattern, doc)))\n",
        "       lang = detect(' '.join(doc_words))\n",
        "       if lang == 'en':\n",
        "           lemmatizer = WordNetLemmatizer()\n",
        "       else:\n",
        "           lemmatizer = FrenchLefffLemmatizer()\n",
        "       return [lemmatizer.lemmatize(word,pos =get_wordnet_pos(word)) for word in doc_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tJypvj7ZnfWY"
      },
      "outputs": [],
      "source": [
        "class NB():\n",
        "  def __init__(self, max_features = 3000, token_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", min_df = 1, max_df = 1.0, ngram_range = (1,1)):\n",
        "    spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "    nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "    self.pattern = token_pattern\n",
        "    self.ngram_range = ngram_range\n",
        "    self.vectorizer = CountVectorizer(binary = True, max_features = max_features, token_pattern = self.pattern, min_df = min_df, max_df = max_df, ngram_range = ngram_range)\n",
        "    self.no_pred = []\n",
        "\n",
        "  # currently train_corpus expects a np array containing a list of strings, not array of words, nxm\n",
        "  # test_label is an nx1 array\n",
        "  def train(self, train_corpus, test_labels):\n",
        "    assert len(train_corpus)  == len(test_labels)\n",
        "    assert type(train_corpus) == type(test_labels) == np.ndarray\n",
        "\n",
        "    vectors_train       = self.vectorizer.fit_transform(train_corpus).todense()\n",
        "    self.analyzer       = self.vectorizer.build_analyzer()\n",
        "    self.word_list      = self.vectorizer.get_feature_names_out()\n",
        "    self.num_samples    = len(test_labels)\n",
        "    self.word_count     = dict(zip(self.word_list, np.array(vectors_train.sum(axis=0))[0]))\n",
        "\n",
        "    unique, counts   = np.unique(test_labels, return_counts=True)\n",
        "    self.label_count = dict(zip(unique, counts))\n",
        "    self.labels      = unique\n",
        "\n",
        "    self.word_count_given_label = {}\n",
        "    for label in self.labels:\n",
        "      indices = np.where(test_labels == label)[0]\n",
        "      tot_word_count = np.array(vectors_train[indices].sum(axis=0))[0]\n",
        "      self.word_count_given_label[label] = {self.word_list[i] : tot_word_count[i] for i in range(len(tot_word_count))}\n",
        "      # print(label, self.word_count_given_label[label])\n",
        "\n",
        "  # Assuming test_corpus is 2-d array where each test sample is a string\n",
        "  def predict(self, test_corpus):\n",
        "\n",
        "    predictions = []\n",
        "    for corpus in tqdm(test_corpus, desc=\"Loading…\", ascii=False, ncols=75):\n",
        "\n",
        "      best_label = ''\n",
        "      best_prob  = -np.inf\n",
        "\n",
        "      corpus_words = self.analyzer(corpus)\n",
        "\n",
        "      for label in self.labels:\n",
        "        p_of_y = self.label_count[label]/self.num_samples\n",
        "\n",
        "        p_of_x_given_y = 1\n",
        "\n",
        "        for word in self.word_list:\n",
        "          if word in corpus_words:\n",
        "            xj = 1\n",
        "          else:\n",
        "            xj = 0\n",
        "\n",
        "          theta_xj_k      = (self.word_count_given_label[label][word] + 1) / (self.label_count[label] + len(self.labels))\n",
        "          p_of_x_given_y *= (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj))\n",
        "          # print(label, word, p_of_x_given_y, theta_xj_k, (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj)))\n",
        "\n",
        "        unseen_words = [new_word for new_word in corpus_words if new_word not in self.word_list]\n",
        "        for word in unseen_words:\n",
        "          # p_of_x_given_y *= 1/(self.label_count[label] + len(self.labels))\n",
        "          p_of_x_given_y *= 1/(len(self.labels)) # When there are a large number of unseen words, the p_of_x_given_y basically becomes 0\n",
        "        # print(len(unseen_words), len(corpus_words))\n",
        "\n",
        "        p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
        "        # print(p_of_y, p_of_x_given_y)\n",
        "        # print(label, p_of_y_given_x)\n",
        "\n",
        "        # print(p_of_y_given_x, best_prob)\n",
        "        if p_of_y_given_x > best_prob:\n",
        "          best_prob = p_of_y_given_x\n",
        "          best_label = label\n",
        "\n",
        "      if p_of_x_given_y == 0:\n",
        "        self.no_pred.append(corpus)\n",
        "      #   best_label = random.choice(self.labels)\n",
        "\n",
        "      predictions.append(best_label)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "tb85vAgDPPdC"
      },
      "outputs": [],
      "source": [
        "def k_fold_validation(x_train_data, y_train_data, K = 10):\n",
        "  assert type(x_train_data) == np.ndarray and type(y_train_data) == np.ndarray\n",
        "\n",
        "  len_of_data  = len(x_train_data) - len(x_train_data) % K\n",
        "  data_x       = x_train_data[:len_of_data]\n",
        "  data_y       = y_train_data[:len_of_data]\n",
        "  size_of_fold = len(x_train_data[:len_of_data]) // K\n",
        "  validation_error = 0\n",
        "\n",
        "  error    = []\n",
        "  pred     = []\n",
        "  timings  = []\n",
        "  no_preds = []\n",
        "\n",
        "  naive_bayes = NB()\n",
        "\n",
        "  for i in range(K):\n",
        "\n",
        "    naive_bayes = NB()\n",
        "\n",
        "    if i != K-1:\n",
        "      validation_fold_x = data_x[i*size_of_fold:(i+1)*size_of_fold]\n",
        "      validation_fold_y = data_y[i*size_of_fold:(i+1)*size_of_fold]\n",
        "\n",
        "      training_folds_x  = np.concatenate((data_x[:i*size_of_fold], data_x[(i+1)*size_of_fold:]))\n",
        "      training_folds_y  = np.concatenate((data_y[:i*size_of_fold], data_y[(i+1)*size_of_fold:]))\n",
        "\n",
        "    else:\n",
        "      validation_fold_x = data_x[i*size_of_fold:]\n",
        "      validation_fold_y = data_y[i*size_of_fold:]\n",
        "\n",
        "      training_folds_x  = data_x[:i*size_of_fold]\n",
        "      training_folds_y  = data_y[:i*size_of_fold]\n",
        "\n",
        "    start_time = time.time()\n",
        "    naive_bayes.train(training_folds_x, training_folds_y)\n",
        "    end_time = time.time()\n",
        "\n",
        "    timings.append(end_time - start_time)\n",
        "\n",
        "    pred_valid = naive_bayes.predict(validation_fold_x)\n",
        "    no_pred = naive_bayes.no_pred\n",
        "    # pred_train = naive_bayes.predict(training_folds_x)\n",
        "\n",
        "    fold_error = {}\n",
        "    fold_error['validation'] = 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    validation_error        += 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    # fold_error['train']      = 1 - accuracy_score(pred_train, training_folds_y)\n",
        "\n",
        "    model_pred = {}\n",
        "    model_pred['validation'] = (pred_valid, validation_fold_y)\n",
        "    # model_pred['train']      = (pred_train, training_folds_y )\n",
        "\n",
        "    error.append(fold_error)\n",
        "    pred.append(model_pred)\n",
        "    no_preds.append(no_pred)\n",
        "\n",
        "  info = {'error': error, 'pred': pred, 'time': timings, 'no_preds' : no_preds}\n",
        "\n",
        "  return validation_error/K, info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xBVjFXWpw7p"
      },
      "source": [
        "# Simple test cases for NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo7BKNgek7NV",
        "outputId": "187b8e04-9372-4bb1-ce65-5fe296f3d41d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|████████████████████████████| 1/1 [00:00<00:00, 1860.83it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['1']"
            ]
          },
          "execution_count": 81,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "     'Is this the first document?',]\n",
        "test_labels = np.array(['1', '2', '3', '4'])\n",
        "train_corpus = np.array(train_corpus)\n",
        "test_corpus = ['harro first pink haha']\n",
        "model = NB()\n",
        "model.train(train_corpus, test_labels)\n",
        "model.predict(test_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jmTZkw2lgUe",
        "outputId": "e170bad5-c51b-4471-86cc-dab6122f8e7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|████████████████████████████| 3/3 [00:00<00:00, 2381.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0, 1, 0]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Training data\n",
        "X_train = np.array([\n",
        "    \"I love this movie\",\n",
        "    \"This movie is great\",\n",
        "    \"A movie like this is great\",\n",
        "    \"I hate this movie\",\n",
        "    \"This movie is terrible\"])\n",
        "\n",
        "# Corresponding labels\n",
        "y_train = np.array([1, 1, 1, 0, 0])  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "model = NB()\n",
        "model.train(X_train, y_train)\n",
        "X_test = [\n",
        "    \"I love this movie terrible\", # Example of both class being viable but the second being chosen due to label eval order in NB\n",
        "    \"I hate this great movie\",\n",
        "    \"This movie is terrible\"]\n",
        "\n",
        "print(model.predict(X_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlHr32RYp0Fn"
      },
      "source": [
        "# K-fold for NB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTGtkcsZM7FP"
      },
      "outputs": [],
      "source": [
        "# TODO: No predictions given, divide by zero warnings might be the cause (Solved?)\n",
        "# TODO: Test changing the prob caluclation to be a sum of logs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVQVLywgFjE1",
        "outputId": "0472baa4-b494-45d2-cd8d-49f53b121b01"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…:  27%|███████▍                   | 38/139 [00:04<00:14,  7.16it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 139/139 [00:11<00:00, 11.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.5755395683453237\n",
            "\n",
            "['in my own experience personal experience (mid 40\\'s m gay) i haven\\'t seen what you\\'ve described often. although unless it\\'s blatantly obvious and annoying i\\'d very likely be oblivious to it, plus it would be much less likely to be directed at me as a male.  other people\\'s experiences particularly that of women may differ significantly, so don\\'t think i\\'m downplaying sexual harassment or verbal abuse!\\n\\ni have witnessed behaviour similar to your descriptions at times in the past in public, private and workplace settings.  if people wish to \\'perv\\' usually they more discreet with any \\'casual observations\\' rather than openly leering while practically drooling everywhere like an over excited dog \\n\\n\\nbelow are several (hopefully) plausible explanations that may (or may not) assist you:\\n\\n\\n*this list does not cover all possible solutions.  it provides some scenarios with possible outcomes.  all of which is solely relying on my personal experience, knowledge and opinions.  it may or may not be suitable or useful for others of any gender.\\n\\n**any improvements i\\'ll happily listen to, but if you\\'re just going to nitpick and be a shitty person with any useful input.... sod off.\\n\\n\\nthere some conditions that affect the mind and/or the brain directly that lower inhibitions and lower or eliminate social conditioning (how we act due to our perception of social norms and expectations).  they can affect anyone of any age but are more prevalent in the elderly.  not every elderly people is effected by these conditions but it may explain a small percentage of the experiences.\\n\\nsignificantly more likely is they are simply being really crappy humans who think that because of their age they are owed something, can freely disregard any and all common decency and should receive a free pass to say or behave in any manner, in any place as they see fit regardless of the impact on others.\\n\\nas much as i don\\'t want to be rude or disrespectful to other cultures, their practices and traditions there could (and i stress could) be an explanation.  this is applicable only if most or all of the interactions with the individuals in question where of the same cultural background.  there are some practices in other cultures that are considered highly inappropriate or extremely disrespectful when viewed by or said/done to an australian or someone of a similar cultural background (ie. generally nw european).\\n\\nit\\'s also worth pointing out that their behaviour could be considered inappropriate or disrespectful even in their own culture but they are prepared act in that manner because they don\\'t believe people will pull them up for it because it\\'s far from their original home.\\n\\nif you don\\'t feel threatened by any of the people who are being problems, feel free to speak up and tell them to \"bugger off\".  if you do feel threatened ever call 000, make a show of it when you do.  that should deter most perverts/harassers and make sure at least one of you discretely starts to record video or audio of th']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|██████████████████████████| 126/126 [00:06<00:00, 18.31it/s]\n",
            "Loading…:  43%|███████████▌               | 54/126 [00:05<00:07,  9.08it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:10<00:00, 12.29it/s]\n",
            "Loading…:  96%|████████████████████████▉ | 121/126 [00:07<00:00, 20.42it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:08<00:00, 14.91it/s]\n",
            "Loading…:  27%|███████▎                   | 34/126 [00:01<00:04, 21.77it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:06<00:00, 20.62it/s]\n",
            "Loading…:  11%|███                        | 14/126 [00:00<00:05, 21.38it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:08<00:00, 15.06it/s]\n",
            "Loading…:  10%|██▌                        | 12/126 [00:00<00:06, 18.88it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:06<00:00, 19.53it/s]\n",
            "Loading…:  15%|████                       | 19/126 [00:01<00:07, 13.77it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:08<00:00, 14.11it/s]\n",
            "Loading…:  47%|████████████▋              | 59/126 [00:03<00:05, 12.74it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:08<00:00, 14.42it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:07<00:00, 16.94it/s]\n",
            "Loading…:   6%|█▊                          | 8/126 [00:00<00:05, 21.89it/s]<ipython-input-80-ccddaf706752>:65: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:08<00:00, 14.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Error: 0.44682539682539685\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# No preprocessing\n",
        "test_split = 0.1\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "\n",
        "model = NB()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "print((pred == testing_labels_split).sum()/len(testing_labels_split))\n",
        "print()\n",
        "print(model.no_pred)\n",
        "\n",
        "mean_error, info = k_fold_validation(training_data_split, training_labels_split)\n",
        "print('Mean Error: {}'.format(mean_error))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbhToUzaVp1x",
        "outputId": "e53254e4-d795-4cf0-b6b6-1d9d794fab74"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|██████████████████████████| 139/139 [00:10<00:00, 12.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6762589928057554\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 35.17it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 22.17it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 33.82it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 35.34it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 23.80it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 34.05it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 32.01it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 23.93it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:04<00:00, 29.87it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 32.60it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mean Error: 0.3841269841269841\n",
            "\n",
            "\n",
            "Model Misses: []\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# With Processing\n",
        "spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "other_stop_words = ['http', 'https', 'www']\n",
        "stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list, other_stop_words))\n",
        "\n",
        "processed_training_data = preprocess(training_data, regex_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", stop_words = stop_words, is_lemmatize = False)\n",
        "\n",
        "test_split = 0.1\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(processed_training_data, training_labels,  test_size = int(len(processed_training_data)*test_split), random_state=seed)\n",
        "\n",
        "model = NB()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "print((pred == testing_labels_split).sum()/len(testing_labels_split))\n",
        "\n",
        "mean_error, info = k_fold_validation(training_data_split, training_labels_split)\n",
        "print()\n",
        "print('Mean Error: {}'.format(mean_error))\n",
        "print()\n",
        "print('\\nModel Misses: {}'.format(model.no_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1udosiGb4te",
        "outputId": "00b35923-6c15-405a-ab09-8001c647e98d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|██████████████████████████| 139/139 [00:09<00:00, 14.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6546762589928058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading…: 100%|██████████████████████████| 126/126 [00:04<00:00, 31.47it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 24.67it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 33.78it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 36.13it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 23.75it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 34.22it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 32.50it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:05<00:00, 22.96it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 32.55it/s]\n",
            "Loading…: 100%|██████████████████████████| 126/126 [00:03<00:00, 33.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mean Error: 0.38015873015873014\n",
            "\n",
            "\n",
            "Model Misses: []\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# With Processing\n",
        "# lemmatized_data = preprocess(training_data, regex_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", stop_words = None, is_lemmatize = True)\n",
        "spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "other_stop_words = ['http', 'https', 'www']\n",
        "stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list, other_stop_words))\n",
        "\n",
        "processed_training_data = preprocess(lemmatized_data, regex_pattern = r\"(?u)\\b[a-z][a-z]+\\b\", stop_words = stop_words, is_lemmatize = False)\n",
        "\n",
        "test_split = 0.1\n",
        "\n",
        "(training_data_split, testing_data_split, training_labels_split, testing_labels_split) = train_test_split(processed_training_data, training_labels,  test_size = int(len(processed_training_data)*test_split), random_state=seed)\n",
        "\n",
        "model = NB()\n",
        "model.train(training_data_split, training_labels_split)\n",
        "pred = model.predict(testing_data_split)\n",
        "print((pred == testing_labels_split).sum()/len(testing_labels_split))\n",
        "\n",
        "mean_error, info = k_fold_validation(training_data_split, training_labels_split)\n",
        "print()\n",
        "print('Mean Error: {}'.format(mean_error))\n",
        "print()\n",
        "print('\\nModel Misses: {}'.format(model.no_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9JJ-fMkF0O",
        "outputId": "08a49ee5-8741-423f-baca-66dcc98ad979"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('like', 240),\n",
              " ('people', 233),\n",
              " ('time', 177),\n",
              " ('year', 169),\n",
              " ('think', 148),\n",
              " ('mai', 146),\n",
              " ('want', 139),\n",
              " ('place', 138),\n",
              " ('work', 134),\n",
              " ('know', 133),\n",
              " ('use', 133),\n",
              " ('look', 131),\n",
              " ('need', 118),\n",
              " ('thing', 117),\n",
              " ('good', 114),\n",
              " ('london', 113),\n",
              " ('lot', 105),\n",
              " ('way', 104),\n",
              " ('find', 97),\n",
              " ('live', 96),\n",
              " ('come', 94),\n",
              " ('try', 94),\n",
              " ('day', 91),\n",
              " ('city', 86),\n",
              " ('feel', 79),\n",
              " ('new', 78),\n",
              " ('best', 72),\n",
              " ('bad', 71),\n",
              " ('help', 71),\n",
              " ('old', 71),\n",
              " ('great', 70),\n",
              " ('pay', 70),\n",
              " ('bit', 67),\n",
              " ('friend', 65),\n",
              " ('tre', 65),\n",
              " ('faire', 64),\n",
              " ('area', 63),\n",
              " ('home', 62),\n",
              " ('life', 62),\n",
              " ('walk', 62),\n",
              " ('big', 61),\n",
              " ('pari', 61),\n",
              " ('right', 61),\n",
              " ('start', 61),\n",
              " ('house', 59),\n",
              " ('long', 59),\n",
              " ('paris', 59),\n",
              " ('point', 59),\n",
              " ('sure', 59),\n",
              " ('job', 58),\n",
              " ('train', 57),\n",
              " ('ago', 56),\n",
              " ('question', 56),\n",
              " ('love', 55),\n",
              " ('bien', 54),\n",
              " ('end', 54),\n",
              " ('let', 53),\n",
              " ('money', 53),\n",
              " ('probably', 53),\n",
              " ('street', 52),\n",
              " ('ask', 50),\n",
              " ('guy', 50),\n",
              " ('rent', 50),\n",
              " ('actually', 49),\n",
              " ('experience', 49),\n",
              " ('line', 49),\n",
              " ('maybe', 49),\n",
              " ('month', 49),\n",
              " ('little', 48),\n",
              " ('living', 48),\n",
              " ('week', 48),\n",
              " ('change', 47),\n",
              " ('family', 47),\n",
              " ('non', 47),\n",
              " ('small', 47),\n",
              " ('price', 46),\n",
              " ('tr', 46),\n",
              " ('close', 45),\n",
              " ('fuck', 45),\n",
              " ('pr', 45),\n",
              " ('public', 45),\n",
              " ('away', 43),\n",
              " ('buy', 43),\n",
              " ('transport', 43),\n",
              " ('cost', 42),\n",
              " ('service', 42),\n",
              " ('ville', 42),\n",
              " ('able', 41),\n",
              " ('far', 41),\n",
              " ('leave', 41),\n",
              " ('mean', 41),\n",
              " ('nice', 41),\n",
              " ('visit', 41),\n",
              " ('food', 40),\n",
              " ('free', 40),\n",
              " ('high', 40),\n",
              " ('park', 40),\n",
              " ('country', 39),\n",
              " ('expensive', 39),\n",
              " ('happen', 39),\n",
              " ('melbourne', 39),\n",
              " ('night', 39),\n",
              " ('post', 39),\n",
              " ('sound', 39),\n",
              " ('hour', 38),\n",
              " ('left', 38),\n",
              " ('police', 38),\n",
              " ('com', 37),\n",
              " ('drive', 37),\n",
              " ('idea', 37),\n",
              " ('person', 37),\n",
              " ('pretty', 37),\n",
              " ('restaurant', 37),\n",
              " ('worth', 37),\n",
              " ('issue', 36),\n",
              " ('local', 36),\n",
              " ('problem', 36),\n",
              " ('reason', 36),\n",
              " ('station', 36),\n",
              " ('stuff', 36),\n",
              " ('bus', 35),\n",
              " ('couple', 35),\n",
              " ('shop', 35),\n",
              " ('situation', 35),\n",
              " ('talk', 35),\n",
              " ('thought', 35),\n",
              " ('french', 34),\n",
              " ('kid', 34),\n",
              " ('sorry', 34),\n",
              " ('bar', 33),\n",
              " ('door', 33),\n",
              " ('especially', 33),\n",
              " ('france', 33),\n",
              " ('fun', 33),\n",
              " ('hard', 33),\n",
              " ('wait', 33),\n",
              " ('age', 32),\n",
              " ('beaucoup', 32),\n",
              " ('easy', 32),\n",
              " ('enjoy', 32),\n",
              " ('fine', 32),\n",
              " ('fr', 32),\n",
              " ('montreal', 32),\n",
              " ('near', 32),\n",
              " ('open', 32),\n",
              " ('run', 32),\n",
              " ('world', 32),\n",
              " ('bon', 31),\n",
              " ('petit', 31),\n",
              " ('usually', 31),\n",
              " ('company', 30),\n",
              " ('face', 30),\n",
              " ('group', 30),\n",
              " ('hand', 30),\n",
              " ('market', 30),\n",
              " ('moment', 30),\n",
              " ('sell', 30),\n",
              " ('social', 30),\n",
              " ('tait', 30),\n",
              " ('temps', 30),\n",
              " ('expect', 29),\n",
              " ('foi', 29),\n",
              " ('grand', 29),\n",
              " ('hope', 29),\n",
              " ('instead', 29),\n",
              " ('option', 29),\n",
              " ('outside', 29),\n",
              " ('found', 28),\n",
              " ('head', 28),\n",
              " ('kind', 28),\n",
              " ('prix', 28),\n",
              " ('remember', 28),\n",
              " ('road', 28),\n",
              " ('tell', 28),\n",
              " ('understand', 28),\n",
              " ('voir', 28),\n",
              " ('vraiment', 28),\n",
              " ('aller', 27),\n",
              " ('book', 27),\n",
              " ('care', 27),\n",
              " ('contact', 27),\n",
              " ('faut', 27),\n",
              " ('honestly', 27),\n",
              " ('metro', 27),\n",
              " ('monde', 27),\n",
              " ('rien', 27),\n",
              " ('shit', 27),\n",
              " ('young', 27),\n",
              " ('advice', 26),\n",
              " ('apr', 26),\n",
              " ('cause', 26),\n",
              " ('check', 26),\n",
              " ('consider', 26),\n",
              " ('control', 26),\n",
              " ('eat', 26),\n",
              " ('follow', 26),\n",
              " ('important', 26),\n",
              " ('learn', 26),\n",
              " ('location', 26),\n",
              " ('space', 26),\n",
              " ('support', 26),\n",
              " ('turn', 26),\n",
              " ('woman', 26),\n",
              " ('case', 25),\n",
              " ('cheap', 25),\n",
              " ('cher', 25),\n",
              " ('edit', 25),\n",
              " ('housing', 25),\n",
              " ('later', 25),\n",
              " ('low', 25),\n",
              " ('recommend', 25),\n",
              " ('single', 25),\n",
              " ('trop', 25),\n",
              " ('article', 24),\n",
              " ('business', 24),\n",
              " ('coffee', 24),\n",
              " ('definitely', 24),\n",
              " ('include', 24),\n",
              " ('play', 24),\n",
              " ('real', 24),\n",
              " ('term', 24),\n",
              " ('watch', 24),\n",
              " ('yes', 24),\n",
              " ('al', 23),\n",
              " ('allow', 23),\n",
              " ('base', 23),\n",
              " ('building', 23),\n",
              " ('chose', 23),\n",
              " ('club', 23),\n",
              " ('co', 23),\n",
              " ('contre', 23),\n",
              " ('fran', 23),\n",
              " ('interest', 23),\n",
              " ('jamais', 23),\n",
              " ('jour', 23),\n",
              " ('mind', 23),\n",
              " ('minute', 23),\n",
              " ('montr', 23),\n",
              " ('offer', 23),\n",
              " ('parent', 23),\n",
              " ('pick', 23),\n",
              " ('rue', 23),\n",
              " ('school', 23),\n",
              " ('set', 23),\n",
              " ('similar', 23),\n",
              " ('site', 23),\n",
              " ('speak', 23),\n",
              " ('system', 23),\n",
              " ('absolutely', 22),\n",
              " ('afford', 22),\n",
              " ('break', 22),\n",
              " ('central', 22),\n",
              " ('centre', 22),\n",
              " ('child', 22),\n",
              " ('community', 22),\n",
              " ('concern', 22),\n",
              " ('date', 22),\n",
              " ('game', 22),\n",
              " ('phone', 22),\n",
              " ('quartier', 22),\n",
              " ('rer', 22),\n",
              " ('soon', 22),\n",
              " ('store', 22),\n",
              " ('super', 22),\n",
              " ('travel', 22),\n",
              " ('tro', 22),\n",
              " ('unfortunately', 22),\n",
              " ('weekend', 22),\n",
              " ('wrong', 22),\n",
              " ('coup', 21),\n",
              " ('english', 21),\n",
              " ('int', 21),\n",
              " ('likely', 21),\n",
              " ('lol', 21),\n",
              " ('mal', 21),\n",
              " ('mention', 21),\n",
              " ('morning', 21),\n",
              " ('partner', 21),\n",
              " ('quebec', 21),\n",
              " ('res', 21),\n",
              " ('rest', 21),\n",
              " ('saw', 21),\n",
              " ('sort', 21),\n",
              " ('spend', 21),\n",
              " ('state', 21),\n",
              " ('bike', 20),\n",
              " ('class', 20),\n",
              " ('google', 20),\n",
              " ('image', 20),\n",
              " ('message', 20),\n",
              " ('passe', 20),\n",
              " ('plein', 20),\n",
              " ('probl', 20),\n",
              " ('saint', 20),\n",
              " ('share', 20),\n",
              " ('short', 20),\n",
              " ('south', 20),\n",
              " ('spot', 20),\n",
              " ('st', 20),\n",
              " ('suggest', 20),\n",
              " ('town', 20),\n",
              " ('trouve', 20),\n",
              " ('tube', 20),\n",
              " ('action', 19),\n",
              " ('ann', 19),\n",
              " ('busy', 19),\n",
              " ('drink', 19),\n",
              " ('drop', 19),\n",
              " ('east', 19),\n",
              " ('extra', 19),\n",
              " ('hear', 19),\n",
              " ('law', 19),\n",
              " ('list', 19),\n",
              " ('lose', 19),\n",
              " ('luck', 19),\n",
              " ('march', 19),\n",
              " ('meal', 19),\n",
              " ('meet', 19),\n",
              " ('op', 19),\n",
              " ('opinion', 19),\n",
              " ('paid', 19),\n",
              " ('population', 19),\n",
              " ('pull', 19),\n",
              " ('second', 19),\n",
              " ('security', 19),\n",
              " ('sport', 19),\n",
              " ('stay', 19),\n",
              " ('website', 19),\n",
              " ('worry', 19),\n",
              " ('yeah', 19),\n",
              " ('ailleurs', 18),\n",
              " ('air', 18),\n",
              " ('ais', 18),\n",
              " ('answer', 18),\n",
              " ('avoid', 18),\n",
              " ('believe', 18),\n",
              " ('bonne', 18),\n",
              " ('bought', 18),\n",
              " ('buying', 18),\n",
              " ('compare', 18),\n",
              " ('cut', 18),\n",
              " ('euro', 18),\n",
              " ('event', 18),\n",
              " ('flat', 18),\n",
              " ('force', 18),\n",
              " ('genre', 18),\n",
              " ('half', 18),\n",
              " ('history', 18),\n",
              " ('impossible', 18),\n",
              " ('information', 18),\n",
              " ('inside', 18),\n",
              " ('lo', 18),\n",
              " ('million', 18),\n",
              " ('miss', 18),\n",
              " ('oh', 18),\n",
              " ('pub', 18),\n",
              " ('read', 18),\n",
              " ('recently', 18),\n",
              " ('room', 18),\n",
              " ('salary', 18),\n",
              " ('semaine', 18),\n",
              " ('stand', 18),\n",
              " ('thanks', 18),\n",
              " ('traffic', 18),\n",
              " ('type', 18),\n",
              " ('uk', 18),\n",
              " ('add', 17),\n",
              " ('arrive', 17),\n",
              " ('bag', 17),\n",
              " ('cool', 17),\n",
              " ('course', 17),\n",
              " ('cross', 17),\n",
              " ('currently', 17),\n",
              " ('fact', 17),\n",
              " ('general', 17),\n",
              " ('hit', 17),\n",
              " ('large', 17),\n",
              " ('late', 17),\n",
              " ('men', 17),\n",
              " ('middle', 17),\n",
              " ('notice', 17),\n",
              " ('order', 17),\n",
              " ('parking', 17),\n",
              " ('photo', 17),\n",
              " ('plan', 17),\n",
              " ('reddit', 17),\n",
              " ('route', 17),\n",
              " ('save', 17),\n",
              " ('source', 17),\n",
              " ('steal', 17),\n",
              " ('surprised', 17),\n",
              " ('today', 17),\n",
              " ('told', 17),\n",
              " ('trip', 17),\n",
              " ('view', 17),\n",
              " ('voiture', 17),\n",
              " ('wonder', 17),\n",
              " ('apartment', 16),\n",
              " ('appreciate', 16),\n",
              " ('attention', 16),\n",
              " ('bank', 16),\n",
              " ('basically', 16),\n",
              " ('block', 16),\n",
              " ('chance', 16),\n",
              " ('charge', 16),\n",
              " ('cheaper', 16),\n",
              " ('continue', 16),\n",
              " ('decide', 16),\n",
              " ('driver', 16),\n",
              " ('example', 16),\n",
              " ('friendly', 16),\n",
              " ('ground', 16),\n",
              " ('ici', 16),\n",
              " ('level', 16),\n",
              " ('longer', 16),\n",
              " ('main', 16),\n",
              " ('map', 16),\n",
              " ('office', 16),\n",
              " ('ok', 16),\n",
              " ('online', 16),\n",
              " ('owner', 16),\n",
              " ('past', 16),\n",
              " ('plenty', 16),\n",
              " ('pris', 16),\n",
              " ('rare', 16),\n",
              " ('rement', 16),\n",
              " ('rule', 16),\n",
              " ('safe', 16),\n",
              " ('sais', 16),\n",
              " ('step', 16),\n",
              " ('suggestion', 16),\n",
              " ('tour', 16),\n",
              " ('zone', 16),\n",
              " ('account', 15),\n",
              " ('activity', 15),\n",
              " ('advance', 15),\n",
              " ('agree', 15),\n",
              " ('anymore', 15),\n",
              " ('aucun', 15),\n",
              " ('automatically', 15),\n",
              " ('budget', 15),\n",
              " ('cas', 15),\n",
              " ('ch', 15),\n",
              " ('choose', 15),\n",
              " ('code', 15),\n",
              " ('coin', 15),\n",
              " ('complain', 15),\n",
              " ('cover', 15),\n",
              " ('deal', 15),\n",
              " ('earn', 15),\n",
              " ('feed', 15),\n",
              " ('girl', 15),\n",
              " ('happy', 15),\n",
              " ('id', 15),\n",
              " ('imagine', 15),\n",
              " ('income', 15),\n",
              " ('join', 15),\n",
              " ('ligne', 15),\n",
              " ('lunch', 15),\n",
              " ('man', 15),\n",
              " ('pain', 15),\n",
              " ('passer', 15),\n",
              " ('perform', 15),\n",
              " ('porte', 15),\n",
              " ('power', 15),\n",
              " ('prendre', 15),\n",
              " ('property', 15),\n",
              " ('provide', 15),\n",
              " ('reach', 15),\n",
              " ('red', 15),\n",
              " ('sense', 15),\n",
              " ('study', 15),\n",
              " ('ticket', 15),\n",
              " ('tip', 15),\n",
              " ('veux', 15),\n",
              " ('water', 15),\n",
              " ('west', 15),\n",
              " ('amaze', 14),\n",
              " ('american', 14),\n",
              " ('bec', 14),\n",
              " ('bot', 14),\n",
              " ('common', 14),\n",
              " ('completely', 14),\n",
              " ('council', 14),\n",
              " ('court', 14),\n",
              " ('covid', 14),\n",
              " ('demande', 14),\n",
              " ('depend', 14),\n",
              " ('distance', 14),\n",
              " ('early', 14),\n",
              " ('film', 14),\n",
              " ('fit', 14),\n",
              " ('green', 14),\n",
              " ('gym', 14),\n",
              " ('hold', 14),\n",
              " ('huge', 14),\n",
              " ('language', 14),\n",
              " ('matter', 14),\n",
              " ('mieux', 14),\n",
              " ('obviously', 14),\n",
              " ('page', 14),\n",
              " ('path', 14),\n",
              " ('private', 14),\n",
              " ('quiet', 14),\n",
              " ('search', 14),\n",
              " ('send', 14),\n",
              " ('simple', 14),\n",
              " ('sit', 14),\n",
              " ('specific', 14),\n",
              " ('speed', 14),\n",
              " ('student', 14),\n",
              " ('style', 14),\n",
              " ('subreddit', 14),\n",
              " ('tourist', 14),\n",
              " ('ver', 14),\n",
              " ('victoria', 14),\n",
              " ('vie', 14),\n",
              " ('vite', 14),\n",
              " ('vrai', 14),\n",
              " ('weird', 14),\n",
              " ('wish', 14),\n",
              " ('acheter', 13),\n",
              " ('act', 13),\n",
              " ('bring', 13),\n",
              " ('built', 13),\n",
              " ('canada', 13),\n",
              " ('catch', 13),\n",
              " ('client', 13),\n",
              " ('cois', 13),\n",
              " ('crime', 13),\n",
              " ('culture', 13),\n",
              " ('dog', 13),\n",
              " ('easily', 13),\n",
              " ('endroit', 13),\n",
              " ('europe', 13),\n",
              " ('exist', 13),\n",
              " ('explain', 13),\n",
              " ('fill', 13),\n",
              " ('fix', 13),\n",
              " ('future', 13),\n",
              " ('guess', 13),\n",
              " ('hate', 13),\n",
              " ('heard', 13),\n",
              " ('hey', 13),\n",
              " ('hotel', 13),\n",
              " ('info', 13),\n",
              " ('insurance', 13),\n",
              " ('kept', 13),\n",
              " ('kill', 13),\n",
              " ('lack', 13),\n",
              " ('logement', 13),\n",
              " ('manage', 13),\n",
              " ('min', 13),\n",
              " ('mortgage', 13),\n",
              " ('moyen', 13),\n",
              " ('normal', 13),\n",
              " ('note', 13),\n",
              " ('number', 13),\n",
              " ('payer', 13),\n",
              " ('relationship', 13),\n",
              " ('rude', 13),\n",
              " ('seat', 13),\n",
              " ('sign', 13),\n",
              " ('spending', 13),\n",
              " ('staff', 13),\n",
              " ('tax', 13),\n",
              " ('tement', 13),\n",
              " ('tower', 13),\n",
              " ('travail', 13),\n",
              " ('truc', 13),\n",
              " ('ts', 13),\n",
              " ('vote', 13),\n",
              " ('wear', 13),\n",
              " ('ami', 12),\n",
              " ('appartement', 12),\n",
              " ('arr', 12),\n",
              " ('art', 12),\n",
              " ('average', 12),\n",
              " ('body', 12),\n",
              " ('clearly', 12),\n",
              " ('commute', 12),\n",
              " ('compose', 12),\n",
              " ('cook', 12),\n",
              " ('cop', 12),\n",
              " ('curit', 12),\n",
              " ('current', 12),\n",
              " ('decent', 12),\n",
              " ('difference', 12),\n",
              " ('direct', 12),\n",
              " ('doctor', 12),\n",
              " ('enter', 12),\n",
              " ('exemple', 12),\n",
              " ('fancy', 12),\n",
              " ('government', 12),\n",
              " ('health', 12),\n",
              " ('hire', 12),\n",
              " ('holiday', 12),\n",
              " ('hot', 12),\n",
              " ('immediately', 12),\n",
              " ('involve', 12),\n",
              " ('landlord', 12),\n",
              " ('lane', 12),\n",
              " ('light', 12),\n",
              " ('limite', 12),\n",
              " ('load', 12),\n",
              " ('ment', 12),\n",
              " ('minimum', 12),\n",
              " ('moderator', 12),\n",
              " ('mother', 12),\n",
              " ('nouvelle', 12),\n",
              " ('okay', 12),\n",
              " ('org', 12),\n",
              " ('particularly', 12),\n",
              " ('party', 12),\n",
              " ('pass', 12),\n",
              " ('personally', 12),\n",
              " ('probablement', 12),\n",
              " ('push', 12),\n",
              " ('realise', 12),\n",
              " ('reminder', 12),\n",
              " ('replace', 12),\n",
              " ('return', 12),\n",
              " ('saving', 12),\n",
              " ('simply', 12),\n",
              " ('soir', 12),\n",
              " ('suppose', 12),\n",
              " ('syst', 12),\n",
              " ('tram', 12),\n",
              " ('transfer', 12),\n",
              " ('value', 12),\n",
              " ('vois', 12),\n",
              " ('accept', 11),\n",
              " ('airport', 11),\n",
              " ('anti', 11),\n",
              " ('apply', 11),\n",
              " ('asian', 11),\n",
              " ('assume', 11),\n",
              " ('autant', 11),\n",
              " ('bill', 11),\n",
              " ('bonus', 11),\n",
              " ('bottle', 11),\n",
              " ('bref', 11),\n",
              " ('card', 11),\n",
              " ('challenge', 11),\n",
              " ('claim', 11),\n",
              " ('cold', 11),\n",
              " ('compl', 11),\n",
              " ('cr', 11),\n",
              " ('damage', 11),\n",
              " ('demand', 11),\n",
              " ('difficult', 11),\n",
              " ('donner', 11),\n",
              " ('downvote', 11),\n",
              " ('downvoting', 11),\n",
              " ('emergency', 11),\n",
              " ('fa', 11),\n",
              " ('fall', 11),\n",
              " ('focus', 11),\n",
              " ('gonna', 11),\n",
              " ('gros', 11),\n",
              " ('ignore', 11),\n",
              " ('illegal', 11),\n",
              " ('increase', 11),\n",
              " ('lieu', 11),\n",
              " ('loin', 11),\n",
              " ('mate', 11),\n",
              " ('member', 11),\n",
              " ('met', 11),\n",
              " ('nord', 11),\n",
              " ('north', 11),\n",
              " ('opening', 11),\n",
              " ('parisian', 11),\n",
              " ('particulier', 11),\n",
              " ('process', 11),\n",
              " ('professional', 11),\n",
              " ('quelqu', 11),\n",
              " ('quickly', 11),\n",
              " ('rate', 11),\n",
              " ('reasonable', 11),\n",
              " ('reduce', 11),\n",
              " ('remove', 11),\n",
              " ('rental', 11),\n",
              " ('report', 11),\n",
              " ('rich', 11),\n",
              " ('roll', 11),\n",
              " ('seau', 11),\n",
              " ('sens', 11),\n",
              " ('seriously', 11),\n",
              " ('strong', 11),\n",
              " ('summer', 11),\n",
              " ('thousand', 11),\n",
              " ('trouver', 11),\n",
              " ('upvote', 11),\n",
              " ('upvoting', 11),\n",
              " ('vue', 11),\n",
              " ('wage', 11),\n",
              " ('window', 11),\n",
              " ('winter', 11),\n",
              " ('write', 11),\n",
              " ('access', 10),\n",
              " ('accessible', 10),\n",
              " ('actual', 10),\n",
              " ('approach', 10),\n",
              " ('argent', 10),\n",
              " ('australia', 10),\n",
              " ('australian', 10),\n",
              " ('bed', 10),\n",
              " ('behaviour', 10),\n",
              " ('besoin', 10),\n",
              " ('bet', 10),\n",
              " ('black', 10),\n",
              " ('bother', 10),\n",
              " ('breakfast', 10),\n",
              " ('cafe', 10),\n",
              " ('camera', 10),\n",
              " ('canal', 10),\n",
              " ('chill', 10),\n",
              " ('conseille', 10),\n",
              " ('crazy', 10),\n",
              " ('crois', 10),\n",
              " ('crowd', 10),\n",
              " ('derni', 10),\n",
              " ('devrait', 10),\n",
              " ('double', 10),\n",
              " ('dude', 10),\n",
              " ('estate', 10),\n",
              " ('eventually', 10),\n",
              " ('extremely', 10),\n",
              " ('eye', 10),\n",
              " ('fee', 10),\n",
              " ('figure', 10),\n",
              " ('foot', 10),\n",
              " ('gare', 10),\n",
              " ('generally', 10),\n",
              " ('grocery', 10),\n",
              " ('guard', 10),\n",
              " ('hang', 10),\n",
              " ('happens', 10),\n",
              " ('hello', 10),\n",
              " ('impression', 10),\n",
              " ('key', 10),\n",
              " ('knew', 10),\n",
              " ('lie', 10),\n",
              " ('link', 10),\n",
              " ('maison', 10),\n",
              " ('manque', 10),\n",
              " ('multiple', 10),\n",
              " ('music', 10),\n",
              " ('nearly', 10),\n",
              " ('niveau', 10),\n",
              " ('noise', 10),\n",
              " ('overall', 10),\n",
              " ('partout', 10),\n",
              " ('payment', 10),\n",
              " ('plut', 10),\n",
              " ('propri', 10),\n",
              " ('province', 10),\n",
              " ('quality', 10),\n",
              " ('rapport', 10),\n",
              " ('realize', 10),\n",
              " ('regularly', 10),\n",
              " ('respect', 10),\n",
              " ('self', 10),\n",
              " ('shock', 10),\n",
              " ('society', 10),\n",
              " ('solution', 10),\n",
              " ('sp', 10),\n",
              " ('struggle', 10),\n",
              " ('sub', 10),\n",
              " ('sun', 10),\n",
              " ('sunday', 10),\n",
              " ('taire', 10),\n",
              " ('thats', 10),\n",
              " ('thread', 10),\n",
              " ('treat', 10),\n",
              " ('tv', 10),\n",
              " ('video', 10),\n",
              " ('wife', 10),\n",
              " ('word', 10),\n",
              " ('accident', 9),\n",
              " ('agent', 9),\n",
              " ('annoy', 9),\n",
              " ('apparently', 9),\n",
              " ('arriv', 9),\n",
              " ('arrondissement', 9),\n",
              " ('attempt', 9),\n",
              " ('attend', 9),\n",
              " ('available', 9),\n",
              " ('aware', 9),\n",
              " ('beat', 9),\n",
              " ('boat', 9),\n",
              " ('brand', 9),\n",
              " ('broke', 9),\n",
              " ('broken', 9),\n",
              " ('build', 9),\n",
              " ('cash', 9),\n",
              " ('center', 9),\n",
              " ('chat', 9),\n",
              " ('cheese', 9),\n",
              " ('choice', 9),\n",
              " ('clear', 9),\n",
              " ('compte', 9),\n",
              " ('cour', 9),\n",
              " ('depends', 9),\n",
              " ('downtown', 9),\n",
              " ('droit', 9),\n",
              " ('dur', 9),\n",
              " ('effort', 9),\n",
              " ('egg', 9),\n",
              " ('enfant', 9),\n",
              " ('entire', 9),\n",
              " ('esp', 9),\n",
              " ('european', 9),\n",
              " ('ex', 9),\n",
              " ('existe', 9),\n",
              " ('exit', 9),\n",
              " ('exp', 9),\n",
              " ('fairly', 9),\n",
              " ('fan', 9),\n",
              " ('felt', 9),\n",
              " ('fire', 9),\n",
              " ('galement', 9),\n",
              " ('ge', 9),\n",
              " ('hell', 9),\n",
              " ('hopefully', 9),\n",
              " ('land', 9),\n",
              " ('lead', 9),\n",
              " ('legal', 9),\n",
              " ('machine', 9),\n",
              " ('mairie', 9),\n",
              " ('major', 9),\n",
              " ('meant', 9),\n",
              " ('mode', 9),\n",
              " ('mum', 9),\n",
              " ('museum', 9),\n",
              " ('nearby', 9),\n",
              " ('occasion', 9),\n",
              " ('officer', 9),\n",
              " ('oui', 9),\n",
              " ('parisien', 9),\n",
              " ('passport', 9),\n",
              " ('pi', 9),\n",
              " ('poor', 9),\n",
              " ('position', 9),\n",
              " ('product', 9),\n",
              " ('protest', 9),\n",
              " ('ralement', 9),\n",
              " ('ration', 9),\n",
              " ('regret', 9),\n",
              " ('review', 9),\n",
              " ('risk', 9),\n",
              " ('rush', 9),\n",
              " ('sad', 9),\n",
              " ('safety', 9),\n",
              " ('salaire', 9),\n",
              " ('scam', 9),\n",
              " ('settle', 9),\n",
              " ('size', 9),\n",
              " ('special', 9),\n",
              " ('spent', 9),\n",
              " ('square', 9),\n",
              " ('stationnement', 9),\n",
              " ('story', 9),\n",
              " ('straight', 9),\n",
              " ('suburb', 9),\n",
              " ('suspect', 9),\n",
              " ('taient', 9),\n",
              " ('tais', 9),\n",
              " ('taste', 9),\n",
              " ('taxi', 9),\n",
              " ('thank', 9),\n",
              " ('total', 9),\n",
              " ('touch', 9),\n",
              " ('trust', 9),\n",
              " ('uber', 9),\n",
              " ('underground', 9),\n",
              " ('veut', 9),\n",
              " ('vic', 9),\n",
              " ('vis', 9),\n",
              " ('war', 9),\n",
              " ('weather', 9),\n",
              " ('welcome', 9),\n",
              " ('york', 9),\n",
              " ('afternoon', 8),\n",
              " ('app', 8),\n",
              " ('banlieue', 8),\n",
              " ('beau', 8),\n",
              " ('beer', 8),\n",
              " ('boy', 8),\n",
              " ('brain', 8),\n",
              " ('bunch', 8),\n",
              " ('caf', 8),\n",
              " ('capital', 8),\n",
              " ('chicken', 8),\n",
              " ('clothes', 8),\n",
              " ('comfortable', 8),\n",
              " ('comprends', 8),\n",
              " ('connect', 8),\n",
              " ('conversation', 8),\n",
              " ('create', 8),\n",
              " ('cu', 8),\n",
              " ('cup', 8),\n",
              " ('cycliste', 8),\n",
              " ('daily', 8),\n",
              " ('decade', 8),\n",
              " ('delivery', 8),\n",
              " ('despite', 8),\n",
              " ('dinner', 8),\n",
              " ('dollar', 8),\n",
              " ('equipment', 8),\n",
              " ('exact', 8),\n",
              " ('fail', 8),\n",
              " ('fast', 8),\n",
              " ('fight', 8),\n",
              " ('floor', 8),\n",
              " ('fly', 8),\n",
              " ('fond', 8),\n",
              " ('fresh', 8),\n",
              " ('friday', 8),\n",
              " ('fruit', 8),\n",
              " ('funny', 8),\n",
              " ('grosse', 8),\n",
              " ('hair', 8),\n",
              " ('heure', 8),\n",
              " ('hole', 8),\n",
              " ('honn', 8),\n",
              " ('ie', 8),\n",
              " ('im', 8),\n",
              " ('immigration', 8),\n",
              " ('immobilier', 8),\n",
              " ('improve', 8),\n",
              " ('indicate', 8),\n",
              " ('industry', 8),\n",
              " ('intersection', 8),\n",
              " ('item', 8),\n",
              " ('journ', 8),\n",
              " ('lady', 8),\n",
              " ('lib', 8),\n",
              " ('lieux', 8),\n",
              " ('listen', 8),\n",
              " ('literally', 8),\n",
              " ('loyer', 8),\n",
              " ('lumi', 8),\n",
              " ('mainly', 8),\n",
              " ('male', 8),\n",
              " ('massive', 8),\n",
              " ('medium', 8),\n",
              " ('meilleur', 8),\n",
              " ('mental', 8),\n",
              " ('mettre', 8),\n",
              " ('minor', 8),\n",
              " ('model', 8),\n",
              " ('movie', 8),\n",
              " ('nature', 8),\n",
              " ('neighbour', 8),\n",
              " ('news', 8),\n",
              " ('nyc', 8),\n",
              " ('package', 8),\n",
              " ('perfect', 8),\n",
              " ('piece', 8),\n",
              " ('pizza', 8),\n",
              " ('planning', 8),\n",
              " ('plupart', 8),\n",
              " ('political', 8),\n",
              " ('popular', 8),\n",
              " ('pouvoir', 8),\n",
              " ('pre', 8),\n",
              " ('prend', 8),\n",
              " ('present', 8),\n",
              " ('pro', 8),\n",
              " ('project', 8),\n",
              " ('projet', 8),\n",
              " ('proprio', 8),\n",
              " ('quick', 8),\n",
              " ('range', 8),\n",
              " ('ratp', 8),\n",
              " ('refuse', 8),\n",
              " ('regard', 8),\n",
              " ('repayment', 8),\n",
              " ('resident', 8),\n",
              " ('ride', 8),\n",
              " ('rience', 8),\n",
              " ('rise', 8),\n",
              " ('sandwich', 8),\n",
              " ('scar', 8),\n",
              " ('seek', 8),\n",
              " ('shopping', 8),\n",
              " ('specifically', 8),\n",
              " ('stick', 8),\n",
              " ('stress', 8),\n",
              " ('sud', 8),\n",
              " ('suddenly', 8),\n",
              " ('sujet', 8),\n",
              " ('supermarket', 8),\n",
              " ('sweet', 8),\n",
              " ('tat', 8),\n",
              " ('text', 8),\n",
              " ('throw', 8),\n",
              " ('transportation', 8),\n",
              " ('unit', 8),\n",
              " ...]"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sorted(model.word_count.items(), key=lambda item: item[1], reverse = True)[:-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqQEA-SVi8nW"
      },
      "source": [
        "# Experimentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZDeBZcBjApY"
      },
      "source": [
        "## N Grams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blCjGI-AjD3L"
      },
      "source": [
        "## Stop Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwJiulmjHAM"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4-78HfOjnED"
      },
      "source": [
        "## Max Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Giz98mOOps5d"
      },
      "source": [
        "# Actual Test Case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W85aadzmO69b"
      },
      "outputs": [],
      "source": [
        "model = NB()\n",
        "model.train(processed_comment_data, labels)\n",
        "\n",
        "test_df = pd.read_csv(path + 'test.csv', encoding = \"ISO-8859-1\")\n",
        "test_data = test_df['body'].to_numpy()\n",
        "processed_test_data = lemmatize_and_remove_stop_words(test_data)\n",
        "\n",
        "predictions = model.predict(processed_test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHux80f5QvTq"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame({'Subreddit':predictions}).reset_index().rename(columns={\"index\": \"Id\"}).to_csv('results.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otC0i9nsOwL2",
        "outputId": "401f6f98-288f-4818-a540-3113c03e843e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy\n",
        "import time\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
        "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
        "import random\n",
        "\n",
        "seed = 10\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "AVHrOlrVeb0y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18ec55a3-535c-40ce-ad86-2ad73d5c4708"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to find P(Y=k), just do test_labels.where(k)/len(test_labels)\n",
        "# vectos_train will contain all the information I need to create the counting for each word\n",
        "# use the get_feature_names to create a dictionary to map to all counts\n",
        "# this dictionary would be contained in another dict that has the labels as keys\n",
        "# to find P(xj,k), find the indices where test_labels = (k), then find"
      ],
      "metadata": {
        "id": "ILJgBzQUkOfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Handle cases where two examples are permutations of another\n",
        "# TODO: Handle laplace smoothing, i.e. word from corpus not in word list"
      ],
      "metadata": {
        "id": "tyztECAW9XOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worth looking into stripping accents, and sklearn feature extraction libarary including idf extractor\n",
        "# create k-fold for this\n",
        "# clarify input to prediction\n",
        "# verify NB implementation\n",
        "# if using countvectorizer for urls, need to use join to recombine sentence, and need to strip any _"
      ],
      "metadata": {
        "id": "YeTjF93jXFiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#' '.join(map(str, train_corpus[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NFFQb9OohZ-E",
        "outputId": "50fb9767-d88b-420a-cdc8-cb0dce72c562"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is the first document.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Olympus in the Sky/McGill/2024 - Winter/ECSE 551/Data/'\n",
        "df = pd.read_csv(path + 'train.csv', encoding = \"ISO-8859-1\")\n",
        "\n",
        "training_data = df['body'].to_numpy()\n",
        "training_labels = df['subreddit'].to_numpy()\n",
        "\n",
        "indices = np.random.permutation(len(training_data))\n",
        "training_data = training_data[indices]\n",
        "training_labels = training_labels[indices]\n",
        "\n",
        "test_split = 0.2\n",
        "\n",
        "(training_data, testing_data, training_labels, testing_labels) = train_test_split(training_data, training_labels,  test_size = int(len(training_data)*test_split), random_state=seed)\n",
        "\n",
        "spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')"
      ],
      "metadata": {
        "id": "FKdP9D1nO2SM"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NB():\n",
        "  def __init__(self):\n",
        "    spacy_stopwords_list = list(fr_stop) + list(en_stop)\n",
        "    nltk_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
        "    self.vectorizer = CountVectorizer(binary = True, max_features = 3000, stop_words = list(set().union(spacy_stopwords_list, nltk_stopwords_list)))\n",
        "\n",
        "  # currently train_corpus expects a np array containing a list of strings, not array of words, nxm\n",
        "  # test_label is an nx1 array\n",
        "  def train(self, train_corpus, test_labels):\n",
        "    assert len(train_corpus) == len(test_labels)\n",
        "    assert type(train_corpus) == type(test_labels) == np.ndarray\n",
        "\n",
        "    vectors_train  = self.vectorizer.fit_transform(train_corpus).todense()\n",
        "    self.word_list   = self.vectorizer.get_feature_names_out()\n",
        "    self.num_samples = len(test_labels)\n",
        "    self.word_count = dict(zip(self.word_list, np.array(vectors_train.sum(axis=0))[0]))\n",
        "\n",
        "    unique, counts   = np.unique(test_labels, return_counts=True)\n",
        "    self.label_count = dict(zip(unique, counts))\n",
        "    self.labels      = unique\n",
        "\n",
        "    self.word_count_given_label = {}\n",
        "    for label in self.labels:\n",
        "      indices = np.where(test_labels == label)[0]\n",
        "      tot_word_count = np.array(vectors_train[indices].sum(axis=0))[0]\n",
        "      self.word_count_given_label[label] = {self.word_list[i] : tot_word_count[i] for i in range(len(tot_word_count))}\n",
        "      # print(label, self.word_count_given_label[label])\n",
        "\n",
        "  # Assuming test_corpus is 2-d array where each test sample is an array of tokens\n",
        "  def predict(self, test_corpus):\n",
        "    assert type(test_corpus) == np.ndarray\n",
        "\n",
        "    predictions = []\n",
        "    for corpus in test_corpus:\n",
        "\n",
        "      best_label = ''\n",
        "      best_prob  = -1e9\n",
        "\n",
        "      for label in self.labels:\n",
        "        p_of_y = self.label_count[label]/self.num_samples\n",
        "\n",
        "        p_of_x_given_y = 1\n",
        "\n",
        "        for word in self.word_list:\n",
        "          if word in corpus:\n",
        "            xj = 1\n",
        "          else:\n",
        "            xj = 0\n",
        "\n",
        "          theta_xj_k      = (self.word_count_given_label[label][word] + 1) / (self.label_count[label] + len(self.labels))\n",
        "          p_of_x_given_y *= (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj))\n",
        "          # print(label, word, p_of_x_given_y, theta_xj_k, (theta_xj_k**(xj) * (1-theta_xj_k)**(1-xj)))\n",
        "\n",
        "        unseen_words = [new_word for new_word in corpus if new_word not in self.word_list]\n",
        "        for word in unseen_words:\n",
        "          p_of_x_given_y *= 1/(self.label_count[label] + len(self.labels))\n",
        "\n",
        "        p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
        "        # print(label, p_of_y_given_x)\n",
        "\n",
        "        # print(p_of_y_given_x, best_prob)\n",
        "        if p_of_y_given_x > best_prob:\n",
        "          best_prob = p_of_y_given_x\n",
        "          best_label = label\n",
        "\n",
        "      # if best_label == '':\n",
        "      #   best_label = random.choice(self.labels)\n",
        "\n",
        "      predictions.append(best_label)\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "tJypvj7ZnfWY"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_validation(x_train_data, y_train_data, K = 10):\n",
        "  assert type(x_train_data) == np.ndarray and type(y_train_data) == np.ndarray\n",
        "\n",
        "  len_of_data  = len(x_train_data) - len(x_train_data) % K\n",
        "  data_x       = x_train_data[:len_of_data]\n",
        "  data_y       = y_train_data[:len_of_data]\n",
        "  size_of_fold = len(x_train_data[:len_of_data]) // K\n",
        "  validation_error = 0\n",
        "\n",
        "  error   = []\n",
        "  pred    = []\n",
        "  timings = []\n",
        "\n",
        "  naive_bayes = NB()\n",
        "\n",
        "  for i in range(K):\n",
        "\n",
        "    naive_bayes = NB()\n",
        "\n",
        "    if i != K-1:\n",
        "      validation_fold_x = data_x[i*size_of_fold:(i+1)*size_of_fold]\n",
        "      validation_fold_y = data_y[i*size_of_fold:(i+1)*size_of_fold]\n",
        "\n",
        "      training_folds_x  = np.concatenate((data_x[:i*size_of_fold], data_x[(i+1)*size_of_fold:]))\n",
        "      training_folds_y  = np.concatenate((data_y[:i*size_of_fold], data_y[(i+1)*size_of_fold:]))\n",
        "\n",
        "    else:\n",
        "      validation_fold_x = data_x[i*size_of_fold:]\n",
        "      validation_fold_y = data_y[i*size_of_fold:]\n",
        "\n",
        "      training_folds_x  = data_x[:i*size_of_fold]\n",
        "      training_folds_y  = data_y[:i*size_of_fold]\n",
        "\n",
        "    start_time = time.time()\n",
        "    naive_bayes.train(training_folds_x, training_folds_y)\n",
        "    end_time = time.time()\n",
        "\n",
        "    timings.append(end_time - start_time)\n",
        "\n",
        "    pred_valid = naive_bayes.predict(validation_fold_x)\n",
        "    pred_train = naive_bayes.predict(training_folds_x)\n",
        "\n",
        "    fold_error = {}\n",
        "    fold_error['validation'] = 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    validation_error        += 1 - accuracy_score(pred_valid, validation_fold_y)\n",
        "    fold_error['train']      = 1 - accuracy_score(pred_train, training_folds_y)\n",
        "\n",
        "    model_pred = {}\n",
        "    model_pred['validation'] = (pred_valid, validation_fold_y)\n",
        "    model_pred['train']      = (pred_train, training_folds_y )\n",
        "\n",
        "    error.append(fold_error)\n",
        "    pred.append(model_pred)\n",
        "\n",
        "  info = {'error': error, 'pred': pred, 'time': timings}\n",
        "\n",
        "  return validation_error/K, info"
      ],
      "metadata": {
        "id": "tb85vAgDPPdC"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple test cases for NB"
      ],
      "metadata": {
        "id": "7xBVjFXWpw7p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "     'Is this the first document?',]\n",
        "test_labels = np.array(['1', '2', '3', '4'])\n",
        "train_corpus = np.array(train_corpus)\n",
        "test_corpus = np.array([['harro', 'first', 'pink', 'third',]])\n",
        "model = NB()\n",
        "model.train(train_corpus, test_labels)\n",
        "model.predict(test_corpus)"
      ],
      "metadata": {
        "id": "qo7BKNgek7NV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a0e34b8-0359-4d35-9817-3905ce33b56e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['3']"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training data\n",
        "X_train = np.array([\n",
        "    \"I love this movie\",\n",
        "    \"This movie is great\",\n",
        "    \"A movie like this is great\",\n",
        "    \"I hate this movie\",\n",
        "    \"This movie is terrible\"\n",
        "])\n",
        "\n",
        "# Corresponding labels\n",
        "y_train = np.array([1, 1, 1, 0, 0])  # 1 for positive sentiment, 0 for negative sentiment\n",
        "\n",
        "\n",
        "model = NB()\n",
        "model.train(X_train, y_train)\n",
        "X_test = np.array([\n",
        "    \"I love this movie, but terrible in some parts d dfa fa sf asdf adf dasf asdf df df dfd\",\n",
        "    \"I hate this great movie\",\n",
        "    \"This movie is terrible\"\n",
        "])\n",
        "\n",
        "\n",
        "for instance in X_test:\n",
        "  print(model.predict(np.array([instance.split()])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jmTZkw2lgUe",
        "outputId": "f7ee80e6-2462-4aa1-cd89-9aad16e9f8ff"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0]\n",
            "[1]\n",
            "[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K-fold for NB"
      ],
      "metadata": {
        "id": "PlHr32RYp0Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_error, info = k_fold_validation(training_data, training_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbhJpJG3p1rR",
        "outputId": "712f782a-9bb8-4351-b2de-142cc9418651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "<ipython-input-54-af378cc40622>:58: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "<ipython-input-54-af378cc40622>:58: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "<ipython-input-54-af378cc40622>:58: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:409: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['neuf', 'quelqu'] not in stop_words.\n",
            "  warnings.warn(\n",
            "<ipython-input-54-af378cc40622>:58: RuntimeWarning: divide by zero encountered in log\n",
            "  p_of_y_given_x = np.log(p_of_y * p_of_x_given_y)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actual Test Case"
      ],
      "metadata": {
        "id": "Giz98mOOps5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = NB()\n",
        "model.train(training_data, training_labels)\n",
        "\n",
        "test_df = pd.read_csv(path + 'test.csv', encoding = \"ISO-8859-1\")\n",
        "test_data = test_df['body'].apply(lambda x: x.lower().split()).to_numpy()\n",
        "\n",
        "predictions = model.predict(test_data)"
      ],
      "metadata": {
        "id": "W85aadzmO69b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'Subreddit':predictions}).reset_index().rename(columns={\"index\": \"Id\"}).to_csv('results.csv', index=False)"
      ],
      "metadata": {
        "id": "YHux80f5QvTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "2gOAtTukpq3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordninja\n",
        "import wordninja"
      ],
      "metadata": {
        "id": "Gj5I-SXfTlDL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(wordninja.split('This document is the spider-man document,egxnd3mtd2l6lxnlcnaih0j1cybhbmqgq29hy2ggq2hhcnrlcibtzwxib3vybmuybhaagbyyhjileaaygaqyiguyhgmycxaagiaegiofgiydmgsqabiabbikbrigazileaaygaqyiguyhgmycxaagiaegiofgiydsoobuksdwkkbcaf4azabajgbnwkgaewnqgedmi04uaedyaea.'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Ci1o7tAvUHpO",
        "outputId": "145f66a3-cd8f-4d97-bde9-8b4a1ca01d53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This document is the spider man document eg x nd 3 m td 2 l 6 l xn lc nai h 0 j 1 cy bh bm qg q 29 hy 2 g gq 2 h hcn rlc ibt zw xi b 3 vy b muy b haag by y hj ilea a yg aq yi guy hg my cx a agia eg i of gi yd mg sq abi abb ik brig a zile a a yg aq yi guy hg my cx a agia eg i of gi yd soo buk s dw k kb caf 4 az abaj gb nw kg a ew n qg edm i 04 uae dy a ea'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(model.word_count.items(), key=lambda item: item[1], reverse = True)[:-1]"
      ],
      "metadata": {
        "id": "UyKywPkeZVxf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}